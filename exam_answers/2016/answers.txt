QUESTION 1:

(a)
Dark silicon is a solution to the problem of heat dissipation and power consumption, but
since we are changing how a CPU is designed, to handle a problem that will only increase with
time, we also need to change the characteristics of that CPU - most notably how to effectively
manage dark silicon to continue to improve computation according to Moore's Law.
	
Chip and system designers will be forced to adopt new asynchrounous circuits that help them
address the power consumption issue. Dedicated co-processors, e.g. the Floating Point Unit
(FPU), SIMD coprocessors, (ARMv8 even has instructions dedicated to AES encryption) have 
been introduced to reduce the pressure on the overall circuitry. If complex jobs can be 
delegated to specialized areas of computing it reduces the burden of power.

The problems that arise with dark silicon primarily revolve around management. There will be 
overhead introduced to manage these asynchronous circuits and different architectures will 
outperform others. There is an optimal dark silicon solution, though that has to be discovered
first.

This is the trend that we've seen in response to the dark silicon. Future processors will 
have more cores/other features, that will be specialized to compute a certain limited range 
of tasks quickly, and using limited power. 

i.e. The floating-point unit consumes very little power when you're not executing floating-
point instructions; when you are, the FPU executes them a lot faster and more 
power-efficiently.

Overall the solution is to have dedicated asynchrounous coprocessors. For example, the GPU
and other collections of coprocessor for image, video and sound processing. 

ARMS big.LITTLE designs are a simple case of heterogenous multicore. They have a 'LITTLE'
processor for maximum power efficiency and while the 'big' processors provide maximum
compute performance. Both are coherent and share the same instruction set.

(b)
Compare and swap ensures atomicity by testing the variable we want to manipulate with an 
expected value for that variable.

In this example the variable that we want to manipulate is at the address pointed to by 
'address', we load this value into oldVal and in one action compare it to an expected value,
'testval'.

If the value is what we expected it to be, then we have access to the correct data, and can 
manipulate it accordingly. If the value is different then we expect that another thread must
have manipulated it and we cannot securely operate on it. This is how the lock is implemented,
if the data is not what we expect then the value is locked. We must then decide how to wait for it to be unlocked, whether the thread spins or dies. 

The high level description shows the concept, but the key part is that this action happens
atomically, that is to say as one indivisible block of code. This is provided at the machine
instruction level. 

However, in compare and swap, a value must be fetched, assessed and potentially changed 
atomically. What prevent another core from accessing the memory address after the first has
fetched it but before it sets the value?

The cache coherency protocol already manages access rights for cache lines. So if a core has
(temporal) exclusive access rights to a cache line, no other core can access that cache line.
To access that cache line the other core has to obtain access rights first, and the protocol
to obtain those rights involves the current owner. In effect, the cache coherency protocol 
prevents other cores from accessing the cache line silently. 

(c)
1.
This code can be vectorized, the body of the for loop deals with a 1D array, or a vector,
meaning it is very easy to vectorize. You could load each of the values into an SSE vector
and use the specialized command for multiplication and addition to perform the operation.

#include xmmintin.h
void add_vect(float *a, float *b, float*c, float factor)
{
	__m128 vfactor = _mm_set1_ps(factor); // load 4 copies of factor.
	for(i=0; i<1024; i+=4){
		__m128 va = _mm_load_ps(&a[i]);
		__m128 vb = _mm_load_ps(&b[i]);
		__m128 vc = _mm_load_ps(&c[i]);
		vx = _mm_mul_ps(vc, vfactor);
		vy = _mm_add_ps(vb, vx);
		_mm_store_ps(&a[i], vy);
	}
}

2.
This code is more difficult to vectorize because of the scalar sum value. However what we can 
do is create 4 partial sums and then sum them horizontally at the end. 

How can we do this if we don't know size is a multiple of 4?

	void add_Vect(float *restrict a, float *restrict b, int size)
	{
		float sum;
		__m128 vsum = _mm_set1_ps(0.0f);
		//assert((n & 3) == 0);						// What do these do?
		//assert(((uintptr_t)a & 15) == 0);
		for(int i=0; i < n; i+=4) {
			__m128 va = _mm_load_ps(&a[i]);
			__m128 vb = _mm_load_ps(&b[i]);
			__m128 vx = _mm_mul_ps(va, vb);
			vsum = mm_add_ps(vsum,vx);
		}
		vsum = _mm_hadd_ps(vsum,vsum);
		vsum = _mm_hadd_ps(vsum,vsum);
		_mm_store_ss(&sum,vsum);
		return sum;
	}



QUESTION 2:
(a)
	void print_duplicates(char **strings, int num_strings) {
		qsort(strings, num_strings, sizeof(char *), strcmp);
		#pragma omp parallel for
		for(int i = 0; i < num_strings; i++) {
			if(strcmp(strings[i], strings[i-1]) == 0)
				printf("%d\n", strings[i]);
		}
	}

(b)
Serial time complexity is O(nlogn).
OMP multithreaded for loop.

Parallelism is implemented using an OpenMP parallel for loop.



QUESTION 3:
(a)
Potential parallelism in the for loop, however, by the end the threads must converge to give
a single value for the sum. 

(b)
1. 
Out of Order Superscalar:
Also called Dynamic Instruction Scheduling, Out of Order Superscalars are processors that can
begin execution of multiple instructions at once. A superscalar will typically have multiple
processing units, (e.g. 2 ALUs) that can execute two instructions at once. In an OoO 
architecture, if an instruction takes a large amount of cycles to execute, other instructions
will be done before it, on condition that they are not dependent on the currently executing
instruction, leading to out of order code execution. 

The above code would see some improvement from an OoO superscalar architecture but that would
mostly just coming from the superscalar aspect.

2.
Very long instruction word(VLIW):
Static multiple-issue processors (aka VLIW) use the compiler to decide which instruction to 
issue and execute simultaneously.

Instructions are always fetched, decoded and issued in pairs. If one instruction of the pair 
can not be used, it is replaced with a noop.

What this means is that ALU operations and Memory Access operations can happen at the same
time. In the case of the above code this means that loading or storing values for diff or
sum will be done concurrently with other instructions. 

We can imagine the code above in (pseudo) Assembly:
	
	LDR r2, &diff
	MUL r3, r2, r2
	STR r2, [&sum]

Under VLIW some of these instruction could execute concurrently, otherwise they would execute
as normal with noops.

3.
Vector Processor:
In computing, a vector processor or array processor is a CPU that implements an instruction
set containing instruction that operate as one-dimensional arrays of data called vectors, 
compared to scalar processors, whose instructions operate on single data items. In order
to be fully optimized by this type of processor the code would have to be vectorized. However
a vector processor could speed up the operation of the line: 
	'double diff = a[i]-mean'
As this is a single operation on a set of data. This is exactly the type of thing a vector 
processor is for; notably array manipulation.

4.
Simultaneous Multithreading:
In simultaneous multithreading, instructions from more than one thread can be executed in any
given pipeline stage at a time. This is done without great changes to the basic processor 
architecture: the main additions needed is the ability to fetch instructions from multiple
threads in a cycle, and a larger register file to hold data from multiple threads. The number
of concurrent threads can be decided by the chip designers. Two concurrent threads per CPU
core are common, but some processors support up to eight concurrent threads per core.

Simultaneous multithreading would have a positive impact on the above code. The ability to 
execute more pipeline stages at once would simply increase efficiency. More instructions 
could be executed in a given time interval. However assuming a large array, many of the
instructions are sequential, such as the summation of diff*diff and rely on the answer from 
previous instructions and so would see negligible gains from multithreading.

5.
Shared Memory Multicore Processor:
A multi-core processor is a single computing component with two or more independent 
processing units called cores, which read and execute program instructions. Often these 
different cores can have different purposes/specializations. This naturally leads to speed
up as multiple instructions are executed at the same time. Problems in this instance arise
from the shared data aspect. When one core has access to a shared variable, the other core 
cannot access it, and may enter spin or die. Shared data means that cache coherency is a
barrier to real speed improvement. For example in the above code, if sum is being written
to in the for loop then the value of it cannot be changed by another core.

6.
Multi Chip Symmetric Processor:
Symmetric multiprocessing (SMP) involves a multiprocessor computer hardware and software
architecture where two or more identical processors are connected to a single, shared 
main memory, have full access to all input and output devices and are controlled by a single
operating system instance that treats all processors equally, reserving none for special
purposes.

The above code would see improvement from this architecture as the Operating system manages the synchronization of different processors. Threads would be scheduled in order to induce
speed-up. In the above code multiple instructions for calculating the standard deviation 
would occur in parallel and then combined. The processors can communicate with each other
via the bus, and the symmetric aspect means that we avoid data corruptions.

7.
NUMA Multi Processor:
Non-uniform memory access (NUMA) is a memory access design where the memory access time
depends on the memory location relative to the processor. Under NUMA, a processor can access
its own local memory faster than non-local memory (memory local to another processory or
memory shared between processors).

The above function is high in data locality the same (or sequential) areads of memory are
repeatedly accessed which means that if a NUMA architecture was used we would have some 
speed up.

8.
Distributed Memory Multicomputer:
In a distributed memory machine every core has access to its own, local, private version of
memory. This can potentially introduce overhead as a memory coherence protocol must be 
implemented, however this excludes race conditions. Distributed memory put the ? on the 
programmer to think about data distribution, however is used properly it can be a secure 
way to implement concurrent programs. While race conditions are eliminated, and data hazards
are avoided, data distribution brings its own set of problems.






















