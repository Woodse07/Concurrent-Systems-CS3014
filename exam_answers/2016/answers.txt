QUESTION 1:

(a)
Dark silicon is a solution to the problem of heat dissipation and power consumption, but
since we are changing how a CPU is designed, to handle a problem that will only increase with
time, we also need to change the characteristics of that CPU - most notably how to effectively
manage dark silicon to continue to improve computation according to Moore's Law.
	
Chip and system designers will be forced to adopt new asynchrounous circuits that help them
address the power consumption issue. Dedicated co-processors, e.g. the Floating Point Unit
(FPU), SIMD coprocessors, (ARMv8 even has instructions dedicated to AES encryption) have 
been introduced to reduce the pressure on the overall circuitry. If complex jobs can be 
delegated to specialized areas of computing it reduces the burden of power.

The problems that arise with dark silicon primarily revolve around management. There will be 
overhead introduced to manage these asynchronous circuits and different architectures will 
outperform others. There is an optimal dark silicon solution, though that has to be discovered
first.

This is the trend that we've seen in response to the dark silicon. Future processors will 
have more cores/other features, that will be specialized to compute a certain limited range 
of tasks quickly, and using limited power. 

i.e. The floating-point unit consumes very little power when you're not executing floating-
point instructions; when you are, the FPU executes them a lot faster and more 
power-efficiently.

Overall the solution is to have dedicated asynchrounous coprocessors. For example, the GPU
and other collections of coprocessor for image, video and sound processing. 

ARMS big.LITTLE designs are a simple case of heterogenous multicore. They have a 'LITTLE'
processor for maximum power efficiency and while the 'big' processors provide maximum
compute performance. Both are coherent and share the same instruction set.

(b)
Compare and swap ensures atomicity by testing the variable we want to manipulate with an 
expected value for that variable.

In this example the variable that we want to manipulate is at the address pointed to by 
'address', we load this value into oldVal and in one action compare it to an expected value,
'testval'.

If the value is what we expected it to be, then we have access to the correct data, and can 
manipulate it accordingly. If the value is different then we expect that another thread must
have manipulated it and we cannot securely operate on it. This is how the lock is implemented,
if the data is not what we expect then the value is locked. We must then decide how to wait for it to be unlocked, whether the thread spins or dies. 

The high level description shows the concept, but the key part is that this action happens
atomically, that is to say as one indivisible block of code. This is provided at the machine
instruction level. 

However, in compare and swap, a value must be fetched, assessed and potentially changed 
atomically. What prevent another core from accessing the memory address after the first has
fetched it but before it sets the value?

The cache coherency protocol already manages access rights for cache lines. So if a core has
(temporal) exclusive access rights to a cache line, no other core can access that cache line.
To access that cache line the other core has to obtain access rights first, and the protocol
to obtain those rights involves the current owner. In effect, the cache coherency protocol 
prevents other cores from accessing the cache line silently. 

(c)
1.
This code can be vectorized, the body of the for loop deals with a 1D array, or a vector,
meaning it is very easy to vectorize. You could load each of the values into an SSE vector
and use the specialized command for multiplication and addition to perform the operation.

#include xmmintin.h
void add_vect(float *a, float *b, float*c, float factor)
{
	__m128 vfactor = _mm_set1_ps(factor); // load 4 copies of factor.
	for(i=0; i<1024; i+=4){
		__m128 va = _mm_load_ps(&a[i]);
		__m128 vb = _mm_load_ps(&b[i]);
		__m128 vc = _mm_load_ps(&c[i]);
		vx = _mm_mul_ps(vc, vfactor);
		vy = _mm_add_ps(vb, vx);
		_mm_store_ps(&a[i], vy);
	}
}

2.
This code is more difficult to vectorize because of the scalar sum value. However what we can 
do is create 4 partial sums and then sum them horizontally at the end. 

How can we do this if we don't know size is a multiple of 4?

	void add_Vect(float *restrict a, float *restrict b, int size)
	{
		float sum;
		__m128 vsum = _mm_set1_ps(0.0f);
		//assert((n & 3) == 0);						// What do these do?
		//assert(((uintptr_t)a & 15) == 0);
		for(int i=0; i < n; i+=4) {
			__m128 va = _mm_load_ps(&a[i]);
			__m128 vb = _mm_load_ps(&b[i]);
			__m128 vx = _mm_mul_ps(va, vb);
			vsum = mm_add_ps(vsum,vx);
		}
		vsum = _mm_hadd_ps(vsum,vsum);
		vsum = _mm_hadd_ps(vsum,vsum);
		_mm_store_ss(&sum,vsum);
		return sum;
	}



QUESTION 2:
(a)
	void print_duplicates(char **strings, int num_strings) {
		qsort(strings, num_strings, sizeof(char *), strcmp);
		#pragma omp parallel for
		for(int i = 0; i < num_strings; i++) {
			if(strcmp(strings[i], strings[i-1]) == 0)
				printf("%d\n", strings[i]);
		}
	}

(b)
Serial time complexity is O(nlogn).
OMP multithreaded for loop.

Parallelism is implemented using an OpenMP parallel for loop.



QUESTION 3:
(a)
Potential parallelism in the for loop, however, by the end the threads must converge to give
a single value for the sum. 

(b)
1. 
Out of Order Superscalar:
Also called Dynamic Instruction Scheduling, Out of Order Superscalars are processors that can
begin execution of multiple instructions at once. A superscalar will typically have multiple
processing units, (e.g. 2 ALUs) that can execute two instructions at once. In an OoO 
architecture, if an instruction takes a large amount of cycles to execute, other instructions
will be done before it, on condition that they are not dependent on the currently executing
instruction, leading to out of order code execution. 

The above code would see some improvement from an OoO superscalar architecture but that would
mostly just coming from the superscalar aspect.

2.
Very long instruction word(VLIW):
Static multiple-issue processors (aka VLIW) use the compiler to decide which instruction to 
issue and execute simultaneously.

Instructions are always fetched, decoded and issued in pairs. If one instruction of the pair 
can not be used, it is replaced with a noop.

What this means is that ALU operations and Memory Access operations can happen at the same
time. In the case of the above code this means that loading or storing values for diff or
sum will be done concurrently with other instructions. 

We can imagine the code above in (pseudo) Assembly:
	
	LDR r2, &diff
	MUL r3, r2, r2
	STR r2, [&sum]

Under VLIW some of these instruction could execute concurrently, otherwise they would execute
as normal with noops.

3.
Vector Processor:
In computing, a vector processor or array processor is a CPU that implements an instruction
set containing instruction that operate as one-dimensional arrays of data called vectors, 
compared to scalar processors, whose instructions operate on single data items. In order
to be fully optimized by this type of processor the code would have to be vectorized. However
a vector processor could speed up the operation of the line: 
	'double diff = a[i]-mean'
As this is a single operation on a set of data. This is exactly the type of thing a vector 
processor is for; notably array manipulation.

4.
Simultaneous Multithreading:








